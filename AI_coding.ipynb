{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code:\n",
    "\n",
    "First, make sure you have alreay exported your OPENAI API key as an environment variable in your terminal.\n",
    "\n",
    "Second, run the following code in the terminal to install openai and the required packages (if this hasn't been done): \n",
    "```bash\n",
    "python.exe -m pip install --upgrade pip   \n",
    "\n",
    "pip install openai     \n",
    "\n",
    "pip install -r install_packages.txt \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from openai import OpenAI\n",
    "import csv\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import chardet # to detect the encoding of the csv file to be converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convert the csv training data, validation data and test data into jsonl file, using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert a csv file into a jsonl file, as the gpt model to be fine-tuned requires jsonl file format as input\n",
    "def csv_to_jsonl(input_csv_path, output_jsonl_path):\n",
    "    # First detect the encoding\n",
    "    with open(input_csv_path, mode='rb') as rawfile:\n",
    "        result = chardet.detect(rawfile.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    # Open the CSV file and create the JSONL file\n",
    "    with open(input_csv_path, mode='r', encoding=encoding) as csv_file, \\\n",
    "        open(output_jsonl_path, mode='w', encoding='utf-8') as jsonl_file:\n",
    "        \n",
    "        # Create a CSV DictReader\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        \n",
    "        # Loop through each row in the CSV\n",
    "        for row in csv_reader:\n",
    "            # Create the structured data for a single conversation\n",
    "            structured_data = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": row['system']},\n",
    "                    {\"role\": \"user\", \"content\": row['user']},\n",
    "                    {\"role\": \"assistant\", \"content\": row['assistant']}\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Convert the structured data to a JSON string and write it to the JSONL file\n",
    "            jsonl_file.write(json.dumps(structured_data,ensure_ascii=False) + '\\n') # ensure_ascii=False ensures that special characters (e.g., Chinese characters) are stored in their original form\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_csv_path = 'path_to_the_csv_training_or_validation_dataset.csv'  # The file path of the csv dataset\n",
    "output_jsonl_path = 'path_to_the_jsonl_training_or_validation_dataset.jsonl' # The jsonl file path where you want to save the file after convertion\n",
    "csv_to_jsonl(input_csv_path, output_jsonl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section is to preprocess and analyze the datasets used for fine-tuning a GPT model. It checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs. \n",
    "\n",
    "For more information, please check the OpenAI notebook page here: https://cookbook.openai.com/examples/chat_finetuning_data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading #\n",
    "\n",
    "data_path = \"path_to_the_dataset.jsonl\" # enter your jsonl file dataset to be checked\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data format validation / Format error checks #\n",
    "\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some token Counting Utilities #\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data warnings and tokens counts #\n",
    "\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing and default n_epochs estimate #\n",
    "\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385 # insert the context window of the base GPT model to be fine-tuned\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carry out a fine-tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the datasets, start a fine-tuning job with the given training and validation dataset\n",
    "using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine-tune a gpt model with a given training data and validation data\n",
    "def fine_tuning_gpt_model(training_data_path, validation_data_path, gpt_model_name, fine_tuned_suffix):\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "\n",
    "\n",
    "    # upload the training data and get its id\n",
    "    training_response = client.files.create(\n",
    "        file=open(training_data_path, \"rb\"), # the path of the training dataset\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "    training_file_id = training_response.id\n",
    "\n",
    "    # upload the validation data and get its id\n",
    "    client = OpenAI()\n",
    "\n",
    "    validation_response = client.files.create(\n",
    "        file=open(validation_data_path, \"rb\"), # the path of the validation dataset\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "    validation_file_id = validation_response.id\n",
    "    \n",
    "\n",
    "    # fine-tune a gpt model\n",
    "    client.fine_tuning.jobs.create(\n",
    "        training_file= training_file_id,  \n",
    "        validation_file= validation_file_id,\n",
    "        model= gpt_model_name,\n",
    "        suffix= fine_tuned_suffix # suffix for the fine-tuned model\n",
    "    )\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling the fine-tuning function, indicate the training data path, validation data path, the name of the gpt model to be fine-tuned, and the suffix for the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "training_data_path = \"path_to_the_training_dataset.jsonl\" # the training data jsonl file path\n",
    "validation_data_path = \"path_to_the_validation_dataset.jsonl\" # the validation data jsonl file path\n",
    "gpt_model_name = \"gpt-3.5-turbo-0125\" # the gpt model to be fine-tuned\n",
    "fine_tuned_suffix = \"suffix_for_the model\" # the suffix to be put in the fine-tuned model's name\n",
    "\n",
    "fine_tuning_gpt_model(training_data_path = training_data_path, validation_data_path = validation_data_path, gpt_model_name = gpt_model_name, fine_tuned_suffix = fine_tuned_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the fine-tuned model and the original gpt model to compare their performances using the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model with the test dataset and write output to a csv file\n",
    "def test_ft_model(model_name, data_path, csv_path):\n",
    "    # Open the CSV file for writing\n",
    "    # \"utf-8-sig\" adds BOM (Byte Order Mark) at the start when writing the csv file, which helps some programs(e.g. Excel) to identify UTF-8 encoding (e.g., Chinese characters).\n",
    "    with open(csv_path, mode='w', newline='', encoding='utf-8-sig') as csv_file:\n",
    "        # Create a CSV writer\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        # Write the header row\n",
    "        csv_writer.writerow(['system', 'user', 'assistant'])\n",
    "        \n",
    "        # Open and read the test data file\n",
    "        with open(data_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Each line is a JSON object representing a conversation instance\n",
    "                conversation = json.loads(line)\n",
    "                # Extract system and user content\n",
    "                system_content = next((msg[\"content\"] for msg in conversation[\"messages\"] if msg[\"role\"] == \"system\"), \"\")\n",
    "                user_content = next((msg[\"content\"] for msg in conversation[\"messages\"] if msg[\"role\"] == \"user\"), \"\")\n",
    "                \n",
    "                # Pass the conversation to the model\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=conversation[\"messages\"]\n",
    "                )\n",
    "                \n",
    "                # Get the model's completion (assistant's response) for the current conversation instance\n",
    "                assistant_content = completion.choices[0].message.content\n",
    "                \n",
    "                # Write the current conversation to the CSV file\n",
    "                csv_writer.writerow([system_content, user_content, assistant_content])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the test dataset (skip this step if it's already prepared in the \"prepare datasets\" step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the test dataset into a jsonl file\n",
    "input_csv_path = \"path_to_the_csv_test_dataset.csv\"\n",
    "output_jsonl_path = \"path_to_the_jsonl_test_dataset.jsonl\"\n",
    "csv_to_jsonl(input_csv_path, output_jsonl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the fine-tuned GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "# The fine_tuned model to be tested\n",
    "model_name = \"ft:gpt-the-fine-tuned-model's-name\" # insert the fine-tuned model's name, which can be found at: https://platform.openai.com/finetune\n",
    "\n",
    "# Path to your test dataset and the output CSV file\n",
    "test_data_path = \"path_to_the_test_dataset.jsonl\"\n",
    "output_csv_path = \"path_to_the_output_dataset.csv\"\n",
    "\n",
    "# Call the function to test the model\n",
    "test_ft_model(model_name, test_data_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the non-fine-tuned base GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "# The original non-fine-tuned gpt model\n",
    "model_name_original = \"gpt-3.5-turbo-0125\" # insert the original gpt model's name\n",
    "\n",
    "# Path to your test dataset and the output CSV file\n",
    "test_data_path = \"path_to_the_test_dataset.jsonl\"\n",
    "output_original_csv_path = \"path_to_the_output_dataset.csv\"\n",
    "\n",
    "# Call the function test the model\n",
    "test_ft_model(model_name_original, test_data_path, output_original_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
